\chapter{Results and Evaluation}
\label{sec:evaluation}

In this section, we describe an automated evaluation framework that measures both the **quality of chatbot responses** (via semantic similarity to an expected answer) and the **retrieval effectiveness** of the underlying vector-based search. The code snippet below outlines how queries, expected answers, and reference data are used to generate numerical scores. Additionally, we present the relevant mathematical formulations to clarify how these scores are computed.

\subsection{Mathematical Foundations}
\label{subsec:math-foundations}

\noindent\textbf{Cosine Similarity.}
Let $\mathbf{x}$ and $\mathbf{y}$ be two $d$-dimensional embeddings (vectors) derived from textual inputs (e.g., a \emph{ground-truth} sentence vs.\ a \emph{chatbot} response). We define the cosine similarity as:
\begin{equation}
  \text{cos\_sim}(\mathbf{x}, \mathbf{y}) \;=\; \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\|\;\|\mathbf{y}\|},
  \label{eq:cosine-sim}
\end{equation}
where $\mathbf{x} \cdot \mathbf{y}$ is the dot product and $\|\mathbf{x}\|$ is the Euclidean norm of $\mathbf{x}$. The result lies in $[-1, 1]$, but for typical sentence embeddings, values are usually in $[0,1]$.

\noindent\textbf{Generation Similarity Score.}
For a given user query $q$ with an \emph{expected} answer $A$, let $R$ denote the chatbot’s \emph{response}. We encode both $A$ and $R$ using the same sentence-transformer embedding function $\Phi(\cdot)$, producing $\Phi(A)$ and $\Phi(R)$. The \emph{generation similarity score} $S_\text{gen}(q)$ is then:
\begin{equation}
  S_\text{gen}(q) \;=\; \text{cos\_sim}\!\Bigl(\Phi(A),\, \Phi(R)\Bigr).
  \label{eq:generation-sim}
\end{equation}
A higher value indicates the chatbot’s generated text is semantically closer to the ground-truth.

\noindent\textbf{Retrieval Similarity Score.}
We also measure how well the system’s retriever (e.g., a vector database) surfaces relevant text chunks. For each query $q$, we fetch the top-$k$ documents $D_1, D_2, \ldots, D_k$ from the retriever. Each document $D_i$ has content that we embed as $\Phi(D_i)$. The \emph{retrieval similarity score} $S_\text{retr}(q)$ is defined as the average similarity of each retrieved chunk to the expected answer embedding $\Phi(A)$:
\begin{equation}
  S_\text{retr}(q) \;=\; \frac{1}{k}\,\sum_{i=1}^{k} \text{cos\_sim}\!\Bigl(\Phi(A),\, \Phi(D_i)\Bigr).
  \label{eq:retrieval-sim}
\end{equation}
If $S_\text{retr}(q)$ is consistently high, it indicates the retriever is pulling documents semantically aligned with the correct answer.

\subsection{Evaluation Code}
\label{subsec:evaluation-code}

\begin{lstlisting}[language=Python, caption={Evaluation Code for Chatbot Responses and Retrieval Quality}, basicstyle=\small\ttfamily]
import os
import time
import logging
import numpy as np
import plotly.graph_objects as go
from sentence_transformers import SentenceTransformer, util

# Import your LLM chain initialization and retrieval functions.
from app import initialize_llm, long_running_task, docsearch, PROMPT

# Set up logging
logging.basicConfig(level=logging.INFO)

# Load a SentenceTransformer model for semantic similarity evaluation.
similarity_model = SentenceTransformer('all-mpnet-base-v2')

def semantic_similarity(expected: str, response: str) -> float:
    """
    Computes the cosine similarity between the expected and response text.
    Returns a float between 0 and 1.
    """
    embedding_expected = similarity_model.encode(expected, convert_to_tensor=True)
    embedding_response = similarity_model.encode(response, convert_to_tensor=True)
    cosine_sim = util.pytorch_cos_sim(embedding_expected, embedding_response)
    return cosine_sim.item()

# Large test dataset based on MedlinePlus diseases.
test_data = [
    {
        "query": "What are the common symptoms of asthma?",
        "expected": "Asthma symptoms include wheezing, shortness of breath, chest tightness, and coughing.",
        "reference": "MedlinePlus. Asthma. https://medlineplus.gov/asthma.html"
    },
    ...
]

def get_chatbot_response(query: str, context: str = "") -> str:
    """
    Runs your QA chain for a given query.
    """
    input_data = {"query": query, "context": context}
    llm = initialize_llm()
    response = long_running_task(input_data, llm)
    return response.strip()

def evaluate_retrieval(query: str, expected: str, k: int = 5) -> float:
    """
    Evaluates retrieval quality by fetching the top k documents for a query
    and computing the average semantic similarity between each document's content
    and the expected answer.
    """
    retriever = docsearch.as_retriever(search_kwargs={'k': k})
    docs = retriever.invoke(query)
    if not docs:
        return 0.0
    scores = [semantic_similarity(expected, doc.page_content) for doc in docs]
    return np.mean(scores)

def evaluate_responses():
    """
    Evaluates chatbot responses against expected answers using semantic similarity,
    and measures retrieval quality.
    Returns lists of similarity scores and retrieval scores.
    """
    similarity_scores = []
    retrieval_scores = []
    
    for test in test_data:
        query = test["query"]
        expected = test["expected"]
        
        # Retrieve chatbot response.
        response = get_chatbot_response(query, context="")
        sim_score = semantic_similarity(expected, response)
        similarity_scores.append(sim_score)
        
        # Evaluate retrieval quality using top-5 documents.
        ret_score = evaluate_retrieval(query, expected, k=5)
        retrieval_scores.append(ret_score)
        
        print(f"Query:                     {query}")
        print(f"Expected Answer:           {expected}")
        print(f"Chatbot Response:          {response}")
        print(f"Semantic Similarity Score (Generation): {sim_score:.2f}")
        print(f"Average Retrieval Similarity Score (top-5): {ret_score:.2f}")
        print("-" * 80)
    
    return similarity_scores, retrieval_scores

def plot_evaluation(similarity_scores, retrieval_scores):
    """
    Uses Plotly to plot the semantic similarity scores for generation and retrieval,
    and saves the graph as a PNG file.
    """
    test_case_indices = list(range(len(test_data)))
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=test_case_indices,
        y=similarity_scores,
        mode='lines+markers',
        name='Generation Similarity'
    ))
    fig.add_trace(go.Scatter(
        x=test_case_indices,
        y=retrieval_scores,
        mode='lines+markers',
        name='Retrieval Similarity'
    ))
    fig.update_layout(
        title="Semantic Similarity Scores per Test Case",
        xaxis_title="Test Case Index",
        yaxis_title="Similarity Score",
        template="plotly_white"
    )
    fig.show()
    # Save the graph as a PNG file
    fig.write_image("evaluation_graph.png")

if __name__ == "__main__":
    # Evaluate responses and collect scores.
    sim_scores, ret_scores = evaluate_responses()
    
    # Plot evaluation metrics using Plotly.
    plot_evaluation(sim_scores, ret_scores)
\end{lstlisting}

\begin{table}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|p{3.5cm}|p{3.8cm}|p{3.8cm}|c|c|}
    \hline
    \textbf{Query} & \textbf{Chatbot Response} & \textbf{Expected Answer} & \textbf{Semantic Similarity} & \textbf{Avg. Retrieval Similarity} \\ 
    \hline
    What are the common symptoms of asthma? 
    & wheezing, shortness of breath, chest tightness
    & wheezing, shortness of breath, chest tightness, coughing 
    & 0.95 
    & 0.70 \\ 
    \hline
    What is anemia and what causes it? 
    & deficiency of red blood cells or hemoglobin; caused by iron deficiency, vitamin deficiency, chronic disease
    & deficiency of red blood cells or hemoglobin commonly caused by iron deficiency, vitamin deficiencies, chronic disease
    & 0.92 
    & 0.70 \\ 
    \hline
    What are the early signs of Alzheimer's disease? 
    & memory loss, confusion, difficulty performing tasks, language problems
    & memory loss, confusion, difficulty performing familiar tasks 
    & 0.94 
    & 0.65 \\ 
    \hline
    What types of arthritis exist and how are they treated? 
    & osteoarthritis, rheumatoid arthritis; treated with anti-inflammatory drugs, pain relievers, DMARDs, physical therapy
    & osteoarthritis, rheumatoid arthritis; treated with pain relievers, anti-inflammatory drugs, physical therapy 
    & 0.92 
    & 0.64 \\ 
    \hline
    What is atrial fibrillation and what are its risks? 
    & irregular heartbeat; risks include stroke, heart failure, blood clots
    & irregular heartbeat; stroke, heart failure, other heart complications 
    & 0.87 
    & 0.68 \\ 
    \hline
    \end{tabular}}
    \label{tab:evaluation-results}
    \caption{Evaluation Results of Chatbot Responses}
    \end{table}
    

\subsection{Workflow and Interpretation}
\begin{enumerate}
    \item \textbf{Load Test Cases.} Each entry in \texttt{test\_data} contains:
    \begin{itemize}
        \item \texttt{query}: The user’s question.
        \item \texttt{expected}: A short, correct answer from medical references.
        \item \texttt{reference}: A citation URL.
    \end{itemize}
    \item \textbf{Generation Similarity.}  
    The system obtains the chatbot’s free-form answer and compares it to the ground-truth (\texttt{expected}) using Equation~\ref{eq:generation-sim}. 
    \begin{equation}
      S_\mathrm{gen} = \text{cos\_sim}\!\Bigl(\Phi(\text{expected}), \,\Phi(\text{response})\Bigr).
      \label{eq:generation-sim}
    \end{equation}
    A higher $S_\mathrm{gen}$ indicates the chatbot’s answer is semantically close to the reference.

    \item \textbf{Retrieval Similarity.}  
    Independently, the code checks the top-$k$ retrieved documents to see how aligned they are with the expected text (Equation~\ref{eq:retrieval-sim}):
    \begin{equation}
      S_\mathrm{retr} = \frac{1}{k} \sum_{i=1}^k \text{cos\_sim}\!\Bigl(\Phi(\text{expected}), \,\Phi(D_i)\Bigr),
      \label{eq:retrieval-sim}
    \end{equation}
    where $D_i$ is the $i$-th retrieved document chunk. A higher $S_\mathrm{retr}$ suggests the vector store is returning relevant context.
\end{enumerate}

\subsection{Significance in the Thesis}
\begin{itemize}
    \item \textbf{Quantitative Insight.} By assigning numerical scores to both the final answer (generation) and the retrieved documents, the methodology pinpoints whether errors stem from the \emph{retrieval} stage or the \emph{generation} stage.
    \item \textbf{Scalability.} New questions can be added to \texttt{test\_data} without retraining the entire model, allowing continuous monitoring of the system’s performance.
    \item \textbf{Guidance for Improvement.} 
    \begin{itemize}
        \item If retrieval scores ($S_\mathrm{retr}$) are consistently high but generation scores ($S_\mathrm{gen}$) are low, the problem may lie in the LLM’s prompt or response generation logic.
        \item If retrieval scores are low, the embedding or vector store configuration might need refinement.
    \end{itemize}
\end{itemize}

\noindent
Overall, this evaluation approach, supported by Equations~\ref{eq:cosine-sim}, \ref{eq:generation-sim}, and \ref{eq:retrieval-sim}, provides a rigorous, automated way to measure how effectively the chatbot answers medical queries and how well its retrieval subsystem surfaces relevant evidence from the knowledge base.

\label{sec:results}

\section{Introduction to Chatbot Interactions}
This section presents selected screenshots that illustrate how our chatbot addresses the research questions posed in this thesis. Each figure demonstrates different functionalities, user roles, and response flows, highlighting how the system handles both student and patient interactions.

\section{Booking an Appointment (Patient Perspective)}
\label{sec:booking-appointment}

\begin{figure}[htbp]
    \centering
    % Adjust the width or use scale if necessary
    \includegraphics[width=0.85\textwidth]{Images/doclink1.png}
    \caption{A patient booking an appointment. The chatbot requests the location (e.g., Munich) and the specialty (e.g., Cardiology), then collects the user’s phone number to send a booking link.}
    \label{fig:book-appointment}
\end{figure}

As shown in Figure~\ref{fig:book-appointment}, the chatbot confirms the patient’s location and medical specialty. Once the user provides a phone number, the system sends a direct link for appointment scheduling, thereby streamlining the booking process.

\section{Student Role: Medical Explanations}
\label{sec:student-role}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{Images/pdfquestion.png}
    \caption{A student asking for detailed medical information. The chatbot provides an academic explanation suited for learning, referencing embedded content.}
    \label{fig:student-chatbot}
\end{figure}

Figure~\ref{fig:student-chatbot} highlights how a user with the \emph{Student} role can request deeper, evidence-based information. The chatbot uses context from embedded academic texts, ensuring that explanations remain precise and pedagogically useful.

\section{Symptom Analysis and Guidance}
\label{sec:patient-symptoms}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{Images/stuentquestion.png}
    \caption{A student asking specific medical questions (e.g., asthma-related issues) in order to learn more. The chatbot responds with context-aware guidance, referencing standard medical guidelines and educational sources.}
    \label{fig:patient-symptoms}
\end{figure}

In Figure~\ref{fig:patient-symptoms}, a patient reports respiratory symptoms. The chatbot, using retrieval-augmented generation (RAG), provides relevant medical guidance while filtering out any sensitive data at the edge to maintain privacy compliance.

\section{Managing Gastrointestinal Issues}
\label{sec:diarrhea-question}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{Images/patientsymptoms.png}
    \caption{A patient scenario discussing diarrhea symptoms for four days. The chatbot suggests dietary measures and advises contacting a healthcare professional if symptoms worsen.}
    \label{fig:diarrhea-question}
\end{figure}

Figure~\ref{fig:diarrhea-question} demonstrates how the chatbot uses user inputs (e.g., “four days of diarrhea”) to provide context-appropriate advice, such as recommending fluid intake and dietary modifications. The system also cautions the user to consult a professional if conditions do not improve.

\section{Provider Recommendations}
\label{sec:doctor-recommendations}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{Images/doctorrecom.png}
    \caption{Chatbot suggesting a list of nearby family doctors in Heidelberg, filtered by location and specialty.}
    \label{fig:doctor-recom}
\end{figure}
In Figure~\ref{fig:doctor-recom}, the chatbot generates a list of nearby healthcare providers based on the patient’s location. This feature demonstrates the system’s ability to integrate location-based data with a specialty filter, simplifying the referral process.


\section{Agentic Booking System}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.3\textwidth]{Images/appointmentlink.png}
  \caption{Agentic AI demonstration: The system automatically integrates with Twilio 
    to send the user an appointment booking link for a cardiology specialist in Munich.}
  \label{fig:agentic_ai_booking}
\end{figure}

\section{Discussion of Chatbot Interaction Results}
Overall, these screenshots illustrate the chatbot’s capacity to:
\begin{itemize}
    \item \textbf{Identify user roles} (patient vs.\ student) and deliver tailored content.
    \item \textbf{Provide medical insights} based on retrieval-augmented generation (RAG).
    \item \textbf{Offer practical guidance} such as appointment booking and provider recommendations.
\end{itemize}

These functionalities address the research questions outlined in the earlier chapters, showcasing how the system can effectively serve both educational and clinical needs in a unified platform.

